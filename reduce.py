# -*- coding: utf-8 -*-
"""reduce.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vq5CfBWNdhE9iXRfPkz_4-dDhZcFrVDJ

Import all the necessary libraries
"""

import sys
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import keras
from keras.optimizers import Adam,RMSprop
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import UpSampling2D, Conv2D, MaxPooling2D, BatchNormalization, Reshape
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from array import *

"""Define 3 functions: 
1.   **read_input** : reads a binary file consisting of images, and returns a numpy array with shape 
(num_images,128,128,1)
2.   **encoder** : constructs the encoded version of the image. 
3. **decoder** : follows the reverse steps from the encoder. 

"""

def read_input(filename):
    global num_images

    f = open(filename, "rb")
    image_size = 28

    #ignore header
    f.read(4)

    num_images = int.from_bytes(f.read(4), 'big')

    row_num = int.from_bytes(f.read(4), 'big')
    col_num = int.from_bytes(f.read(4), 'big')

    buf = f.read(row_num * col_num * num_images)
    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32) / 255
    data = data.reshape(num_images, row_num, col_num, 1)
    return data

enlayers = 0

def encoder(input_img):
    #encoder
    #input = 28 x 28 x 1 (wide and thin)
    global reduced_dimension
    global enlayers 
    enlayers = 6
    reduced_dimension = 10

    enc = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32
    enc = BatchNormalization()(enc)

    enc = MaxPooling2D(pool_size=(2, 2))(enc) # 14 x 14 x 32

    enc = Conv2D(64, (3, 3), activation='relu', padding='same')(enc) #14 x 14 x 64
    enc = BatchNormalization()(enc)

    enc = MaxPooling2D(pool_size=(2, 2))(enc) # 7 x 7 x 64

    enc = Conv2D(128, (3, 3), activation='relu', padding='same')(enc) # 7 x 7 x 128
    enc = BatchNormalization()(enc)

    enc = Flatten()(enc)
    enc = Dense(units = reduced_dimension, activation='relu')(enc)

    return enc
    

def decoder(x):
    #decoder

    x = Dense(units = 6272, activation='relu')(x)
    x = Reshape((7,7,128))(x)

    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x) #7 x 7 x 128
    x = BatchNormalization()(x)

    x = UpSampling2D((2,2))(x) #14 x 14 x 128
    
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x) #14 x 14 x 64
    x = BatchNormalization()(x)   

    x = UpSampling2D((2,2))(x) # 28 x 28 x 64
    
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x) #28 x 28 x 32
    x = BatchNormalization()(x)

    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) # 28 x 28 x 1

    return decoded

"""Read the binary files for test,train and label data.

Normalize data in the [0,1] scale

Seperate the input into train and validation set

Default : 80% train data , 20% validation data
"""

from tensorflow.python.client import device_lib

input_filename = "/content/drive/My Drive/ML/data3/train-images-idx3-ubyte" #for the Colab
query_filename = "/content/drive/MyDrive/ML/data3/t10k-images-idx3-ubyte" #for the Colab
output_dataset_filename = "out_dataset.bin"
output_queryset_filename = "out_queryset.bin"

#For the .py 
if "-d" in sys.argv:
    input_filename = sys.argv[sys.argv.index("-d") + 1]
if "-q" in sys.argv: 
    query_filename = sys.argv[sys.argv.index("-q") + 1]
if "-od" in sys.argv:
    output_dataset_filename = sys.argv[sys.argv.index("-od") + 1]
if "-oq" in sys.argv: 
    output_queryset_filename = sys.argv[sys.argv.index("-oq") + 1]


print(input_filename)
print(query_filename)
print(output_dataset_filename)
print(output_queryset_filename)


num_images = 0 
global dataset_num_images
global queryset_num_images

train_data = read_input(input_filename)
dataset_num_images = num_images
query_data = read_input(query_filename)
queryset_num_images = num_images

print(dataset_num_images, queryset_num_images)
print(train_data.shape)
print(train_data.dtype)

# Normalize data in the 0-1 scale
train_data = train_data / np.max(train_data)


# Use the initial data as a training set and validation set. 
# 80% of the data will be the training set
# 20% of the data will be the validation set
train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,train_data, 
                                                        test_size=0.2, random_state=13)

"""Create a keras model and copile with mean_squared_error loss function and RMSprop optimizer.

Start the fitting process with 50 epochs and 128 batch size as default.

Save the model with .h5 extension and plot the training and validation loss
"""

repeat = True
global enlayers
global reduced_dimension
reduced_dimension = 10

retrain_option = input("If you want to retrain the model press 1: ")
if (int(retrain_option) == 1):
    # model_path = '/content/drive/My Drive/ML/data3/results/'            #for the colab
    model_path = input("Insert the path to save the models: ") #for the .py 

    batch_size = input("Batch size: ")
    epochs = input("Epochs: ")
    batch_size = int(batch_size)
    epochs = int(epochs)
    input_img = keras.Input(shape=(28,28,1))
    autoencoder = keras.Model(input_img, decoder(encoder(input_img))) 
    autoencoder.summary()
    autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())



    # Start the fitting process 
    autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,
                                    epochs=epochs,verbose=1,
                                    validation_data=(valid_X, valid_ground))
    h5name = "layers" + str(enlayers) + "_epochs"+ str(epochs) +"_batch"+ str(batch_size)


    # Visualize train and validation loss
    loss = autoencoder_train.history['loss']
    val_loss = autoencoder_train.history['val_loss']
    epochs = range(epochs)
    plt.figure()
    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.savefig(model_path + h5name + '.png')
    plt.show()

    user_option = input("If you want to save the model, press 1: ")
    if (int(user_option) == 1):
        autoencoder.save(model_path  + h5name + '.h5')

"""**Production of the output**

Extract the layers of the encoder 

Pass the input through it and then save the results in the output file
"""

if (int(retrain_option) != 1):
    # autoencoder = keras.models.load_model('/content/drive/MyDrive/ML/data3/results/layers6_epochs100_batch512.h5') #for colab
    autoencoder = keras.models.load_model(model_path + h5name + '.h5') #make one for .py


autoencoder.summary()


intermediate_layer_model = keras.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(name=None, index=10).output)

#--------Dataset
#--------Getting the reduced vectors-------------
#--------Exporting them to the output file-------

reduced_dataset = intermediate_layer_model.predict(train_data)

max_value = np.max(reduced_dataset)
reduced_dataset = reduced_dataset / max_value * 25500
reduced_dataset = reduced_dataset.astype(int)

outputfile = open(output_dataset_filename, 'wb')

temp = 0 #magic number
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

temp = dataset_num_images #number of images
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

temp = 1 #rows
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

temp = reduced_dimension #cols 
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

total = 0
for i in range(0, dataset_num_images):
    for j in range(0, reduced_dimension):
        temp = int(reduced_dataset[i][j])
        outputfile.write((temp).to_bytes(2, byteorder='little', signed=False))
        total = total + temp

outputfile.close()


#----------------------------


#--------Queryset
#--------Getting the reduced vectors-------------
#--------Exporting them to the output file-------
reduced_queryset = intermediate_layer_model.predict(query_data)

max_value = np.max(reduced_queryset)
reduced_queryset = reduced_queryset / max_value * 25500
reduced_queryset = reduced_queryset.astype(int)

outputfile = open(output_queryset_filename, 'wb')

temp = 0 #magic number
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

temp = queryset_num_images #number of images
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

temp = 1 #rows
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

temp = reduced_dimension #cols 
outputfile.write((temp).to_bytes(4, byteorder='big', signed=False))

total = 0
for i in range(0, queryset_num_images):
    for j in range(0, reduced_dimension):
        temp = int(reduced_queryset[i][j])
        outputfile.write((temp).to_bytes(2, byteorder='little', signed=False))
        total = total + temp


outputfile.close()

#----------------------------